{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\n#import required libraries\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.utils.data as data\n \n# load ascii text and covert to lowercase\nfilename = \"/kaggle/input/ebooksentence/6761-0.txt\"\nraw_text = open(filename, 'r', encoding='utf-8').read()\nraw_text = raw_text.lower()\n \n# create mapping of unique chars to integers\nchars = sorted(list(set(raw_text)))\nchar_to_int = dict((c, i) for i, c in enumerate(chars))\n \n# summarize the loaded data\nn_chars = len(raw_text)\nn_vocab = len(chars)\nprint(\"Total Characters: \", n_chars)\nprint(\"Total Vocab: \", n_vocab)\n \n# prepare the dataset of input to output pairs encoded as integers\nseq_length = 100\ndataX = []\ndataY = []\nfor i in range(0, n_chars - seq_length, 1):\n    seq_in = raw_text[i:i + seq_length]\n    seq_out = raw_text[i + seq_length]\n    dataX.append([char_to_int[char] for char in seq_in])\n    dataY.append(char_to_int[seq_out])\nn_patterns = len(dataX)\nprint(\"Total Patterns: \", n_patterns)\n \n# reshape X to be [samples, time steps, features]\nX = torch.tensor(dataX, dtype=torch.float32).reshape(n_patterns, seq_length, 1)\nX = X / float(n_vocab)\ny = torch.tensor(dataY)\n \nclass CharModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.lstm = nn.LSTM(input_size=1, hidden_size=256, num_layers=2, batch_first=True, dropout=0.2)\n        self.dropout = nn.Dropout(0.2)\n        self.linear = nn.Linear(256, n_vocab)\n    def forward(self, x):\n        x, _ = self.lstm(x)\n        # take only the last output\n        x = x[:, -1, :]\n        # produce output\n        x = self.linear(self.dropout(x))\n        return x\n\n#set the hyperparameters\nn_epochs = 40\nbatch_size = 128\nmodel = CharModel()\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n \noptimizer = optim.Adam(model.parameters())\nloss_fn = nn.CrossEntropyLoss(reduction=\"sum\")\nloader = data.DataLoader(data.TensorDataset(X, y), shuffle=True, batch_size=batch_size)\n \nbest_model = None\nbest_loss = np.inf\n\n#run the model for the given epochs\nfor epoch in range(n_epochs):\n    model.train()\n    for X_batch, y_batch in loader:\n        y_pred = model(X_batch.to(device))\n        loss = loss_fn(y_pred, y_batch.to(device))\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    # Validation\n    model.eval()\n    loss = 0\n    with torch.no_grad():\n        for X_batch, y_batch in loader:\n            y_pred = model(X_batch.to(device))\n            loss += loss_fn(y_pred, y_batch.to(device))\n        if loss < best_loss:\n            best_loss = loss\n            best_model = model.state_dict()\n        print(\"Epoch %d: Cross-entropy: %.4f\" % (epoch, loss))\n \ntorch.save([best_model, char_to_int], \"single-char.pth\")\n \n# Generation using the trained model\nbest_model, char_to_int = torch.load(\"single-char.pth\")\nn_vocab = len(char_to_int)\nint_to_char = dict((i, c) for c, i in char_to_int.items())\nmodel.load_state_dict(best_model)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-04-21T15:03:29.810339Z","iopub.execute_input":"2023-04-21T15:03:29.810775Z","iopub.status.idle":"2023-04-21T17:51:06.782244Z","shell.execute_reply.started":"2023-04-21T15:03:29.810738Z","shell.execute_reply":"2023-04-21T17:51:06.781129Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Total Characters:  974238\nTotal Vocab:  62\nTotal Patterns:  974138\nEpoch 0: Cross-entropy: 2131908.7500\nEpoch 1: Cross-entropy: 1863718.6250\nEpoch 2: Cross-entropy: 1710715.0000\nEpoch 3: Cross-entropy: 1641318.6250\nEpoch 4: Cross-entropy: 1549532.2500\nEpoch 5: Cross-entropy: 1501205.8750\nEpoch 6: Cross-entropy: 1471907.5000\nEpoch 7: Cross-entropy: 1437843.2500\nEpoch 8: Cross-entropy: 1409095.5000\nEpoch 9: Cross-entropy: 1386524.6250\nEpoch 10: Cross-entropy: 1391955.1250\nEpoch 11: Cross-entropy: 1348702.3750\nEpoch 12: Cross-entropy: 1337647.6250\nEpoch 13: Cross-entropy: 1357728.5000\nEpoch 14: Cross-entropy: 1317505.6250\nEpoch 15: Cross-entropy: 1319120.3750\nEpoch 16: Cross-entropy: 1303259.8750\nEpoch 17: Cross-entropy: 1284446.3750\nEpoch 18: Cross-entropy: 1273782.3750\nEpoch 19: Cross-entropy: 1264423.1250\nEpoch 20: Cross-entropy: 1265295.7500\nEpoch 21: Cross-entropy: 1249367.2500\nEpoch 22: Cross-entropy: 1245870.0000\nEpoch 23: Cross-entropy: 1238497.5000\nEpoch 24: Cross-entropy: 1241695.6250\nEpoch 25: Cross-entropy: 1230262.0000\nEpoch 26: Cross-entropy: 1229563.7500\nEpoch 27: Cross-entropy: 1222535.2500\nEpoch 28: Cross-entropy: 1218669.8750\nEpoch 29: Cross-entropy: 1211765.3750\nEpoch 30: Cross-entropy: 1211787.2500\nEpoch 31: Cross-entropy: 1205622.7500\nEpoch 32: Cross-entropy: 1206874.3750\nEpoch 33: Cross-entropy: 1202132.7500\nEpoch 34: Cross-entropy: 1200674.2500\nEpoch 35: Cross-entropy: 1197158.3750\nEpoch 36: Cross-entropy: 1188058.8750\nEpoch 37: Cross-entropy: 1183396.6250\nEpoch 38: Cross-entropy: 1181223.3750\nEpoch 39: Cross-entropy: 1176746.7500\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"code","source":"# randomly generate a prompt\nfilename = \"/kaggle/input/ebooksentence/6761-0.txt\"\nseq_length = 100\nraw_text = open(filename, 'r', encoding='utf-8').read()\nraw_text = raw_text.lower()\nstart = np.random.randint(0, len(raw_text)-seq_length)\nprompt = raw_text[start:start+seq_length]\npattern = [char_to_int[c] for c in prompt]  \n \nmodel.eval()\nprint('Prompt: \"%s\"' % prompt)\nwith torch.no_grad():\n    for i in range(1000):\n        # format input array of int into PyTorch tensor\n        x = np.reshape(pattern, (1, len(pattern), 1)) / float(n_vocab)\n        x = torch.tensor(x, dtype=torch.float32)\n        # generate logits as output from the model\n        prediction = model(x.to(device))\n        # convert logits into one character\n        index = int(prediction.argmax())\n        result = int_to_char[index]\n        print(result, end=\"\")\n        # append the new character into the prompt for the next iteration\n        pattern.append(index)\n        pattern = pattern[1:]\nprint()\nprint(\"Done.\")","metadata":{"execution":{"iopub.status.busy":"2023-04-21T17:51:06.784472Z","iopub.execute_input":"2023-04-21T17:51:06.785056Z","iopub.status.idle":"2023-04-21T17:51:09.149697Z","shell.execute_reply.started":"2023-04-21T17:51:06.785019Z","shell.execute_reply":"2023-04-21T17:51:09.148592Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Prompt: \"een cherished by the conversation of her school-fellows. she\nwas particularly fond of music, in whic\"\nh he was already seen to the project gutenberg-tm electronic work with the service of the service, and the count was not a language to the project gutenberg-tm electronic work and despair, and the count was not a language to the project gutenberg-tm electronic work and despair, and the count was not a language to the project gutenberg-tm electronic work and despair, and the count was not a language to the project gutenberg-tm electronic work and despair, and the count was not a language to the project gutenberg-tm electronic work and despair, and the count was not a language to the project gutenberg-tm electronic work and despair, and the count was not a language to the project gutenberg-tm electronic work and despair, and the count was not a language to the project gutenberg-tm electronic work and despair, and the count was not a language to the project gutenberg-tm electronic work and despair, and the count was not a language to the project gutenberg-tm electronic work and despair, a\nDone.\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-04-19T13:52:19.355512Z","iopub.execute_input":"2023-04-19T13:52:19.356237Z","iopub.status.idle":"2023-04-19T13:52:21.691753Z","shell.execute_reply.started":"2023-04-19T13:52:19.356198Z","shell.execute_reply":"2023-04-19T13:52:21.690642Z"},"trusted":true},"execution_count":1,"outputs":[{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}